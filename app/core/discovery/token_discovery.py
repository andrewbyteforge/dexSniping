"""
Token Discovery Service
File: app/core/discovery/token_discovery.py

Real-time token discovery engine that monitors DEXs for new token launches,
analyzes them with AI, and feeds viable opportunities to the trading bot.
"""

import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass, field
from decimal import Decimal
import aiohttp

from app.utils.logger import setup_logger
from app.core.exceptions import TokenDiscoveryError, ValidationError
from app.core.ai.risk_assessor import AIRiskAssessor
from app.core.ai.honeypot_detector import HoneypotDetector
from app.core.database import get_db_session

logger = setup_logger(__name__)


@dataclass
class DiscoveredToken:
    """Newly discovered token with initial analysis."""
    address: str
    network: str
    symbol: str
    name: str
    decimals: int
    total_supply: Decimal
    creation_block: int
    creation_timestamp: datetime
    creator_address: str
    initial_liquidity_eth: Decimal
    initial_liquidity_usd: float
    current_price_usd: float
    current_liquidity_usd: float
    volume_1h: float
    holder_count: int
    transaction_count: int
    dex_pairs: List[Dict[str, Any]]
    discovery_source: str
    discovered_at: datetime = field(default_factory=datetime.utcnow)
    ai_analyzed: bool = False
    risk_score: Optional[float] = None
    is_honeypot: Optional[bool] = None
    trading_viable: bool = False


@dataclass
class DiscoveryStats:
    """Token discovery statistics."""
    total_discovered: int = 0
    viable_opportunities: int = 0
    honeypots_detected: int = 0
    average_discovery_time: float = 0.0
    networks_monitored: int = 0
    active_monitors: int = 0


class TokenDiscovery:
    """
    Real-time token discovery and analysis engine.
    
    Features:
    - Multi-DEX monitoring (Uniswap, PancakeSwap, etc.)
    - Real-time new pair detection
    - Instant AI risk assessment
    - Honeypot filtering
    - Liquidity analysis
    - Opportunity scoring and ranking
    """
    
    def __init__(self):
        """Initialize token discovery service."""
        self.discovered_tokens: Dict[str, DiscoveredToken] = {}
        self.monitoring_networks: Set[str] = set()
        self.active_monitors: Dict[str, bool] = {}
        self.discovery_stats = DiscoveryStats()
        
        # AI components
        self.risk_assessor: Optional[AIRiskAssessor] = None
        self.honeypot_detector: Optional[HoneypotDetector] = None
        
        # Configuration
        self.min_liquidity_threshold = 5000  # $5k minimum
        self.max_token_age_hours = 24
        self.discovery_interval = 30  # seconds
        self.analysis_batch_size = 10
        
        # Monitoring sources
        self.dex_sources = {
            'ethereum': [
                'uniswap_v2',
                'uniswap_v3', 
                'sushiswap',
                'balancer'
            ],
            'bsc': [
                'pancakeswap_v2',
                'pancakeswap_v3',
                'biswap'
            ],
            'polygon': [
                'quickswap',
                'sushiswap_polygon'
            ]
        }
        
        logger.info("‚úÖ Token Discovery Service initialized")
    
    async def initialize(self) -> bool:
        """Initialize discovery service components."""
        try:
            logger.info("üîç Initializing token discovery service...")
            
            # Initialize AI components
            from app.core.ai.risk_assessor import AIRiskAssessor
            from app.core.ai.honeypot_detector import HoneypotDetector
            
            self.risk_assessor = AIRiskAssessor()
            self.honeypot_detector = HoneypotDetector()
            
            await self.risk_assessor.initialize_models()
            await self.honeypot_detector.initialize()
            
            logger.info("‚úÖ Token discovery service initialized")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize token discovery: {e}")
            return False
    
    async def start_monitoring(self, networks: List[str]) -> None:
        """Start monitoring specified networks for new tokens."""
        try:
            logger.info(f"üöÄ Starting token discovery monitoring for: {networks}")
            
            self.monitoring_networks = set(networks)
            
            # Start monitoring tasks for each network
            tasks = []
            for network in networks:
                if network in self.dex_sources:
                    self.active_monitors[network] = True
                    task = asyncio.create_task(self._monitor_network(network))
                    tasks.append(task)
            
            # Start analysis task
            analysis_task = asyncio.create_task(self._analyze_discovered_tokens())
            tasks.append(analysis_task)
            
            # Wait for all monitoring tasks
            await asyncio.gather(*tasks)
            
        except Exception as e:
            logger.error(f"‚ùå Monitoring failed: {e}")
            raise TokenDiscoveryError(f"Failed to start monitoring: {e}")
    
    async def stop_monitoring(self) -> None:
        """Stop all monitoring activities."""
        logger.info("üõë Stopping token discovery monitoring...")
        
        # Stop all monitors
        for network in self.active_monitors:
            self.active_monitors[network] = False
        
        self.monitoring_networks.clear()
    
    async def get_recent_tokens(
        self,
        network: Optional[str] = None,
        max_age_minutes: int = 60,
        min_liquidity: float = 5000,
        only_viable: bool = True
    ) -> List[Dict[str, Any]]:
        """Get recently discovered tokens matching criteria."""
        try:
            cutoff_time = datetime.utcnow() - timedelta(minutes=max_age_minutes)
            recent_tokens = []
            
            for token in self.discovered_tokens.values():
                # Filter by network
                if network and token.network != network:
                    continue
                
                # Filter by age
                if token.discovered_at < cutoff_time:
                    continue
                
                # Filter by liquidity
                if token.current_liquidity_usd < min_liquidity:
                    continue
                
                # Filter by viability
                if only_viable and not token.trading_viable:
                    continue
                
                token_data = {
                    "address": token.address,
                    "network": token.network,
                    "symbol": token.symbol,
                    "name": token.name,
                    "price_usd": token.current_price_usd,
                    "liquidity_usd": token.current_liquidity_usd,
                    "volume_1h": token.volume_1h,
                    "holder_count": token.holder_count,
                    "age_minutes": int((datetime.utcnow() - token.creation_timestamp).total_seconds() / 60),
                    "discovered_at": token.discovered_at,
                    "risk_score": token.risk_score,
                    "is_honeypot": token.is_honeypot,
                    "trading_viable": token.trading_viable,
                    "discovery_source": token.discovery_source
                }
                
                recent_tokens.append(token_data)
            
            # Sort by discovery time (newest first)
            recent_tokens.sort(key=lambda x: x['discovered_at'], reverse=True)
            
            return recent_tokens
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get recent tokens: {e}")
            return []
    
    async def get_top_opportunities(
        self,
        network: Optional[str] = None,
        limit: int = 20
    ) -> List[Dict[str, Any]]:
        """Get top trading opportunities based on AI scoring."""
        try:
            recent_tokens = await self.get_recent_tokens(network=network, only_viable=True)
            
            # Score and rank opportunities
            scored_opportunities = []
            
            for token in recent_tokens:
                if token['risk_score'] is None:
                    continue
                
                # Calculate opportunity score
                opportunity_score = self._calculate_opportunity_score(token)
                
                token['opportunity_score'] = opportunity_score
                scored_opportunities.append(token)
            
            # Sort by opportunity score (highest first)
            scored_opportunities.sort(key=lambda x: x['opportunity_score'], reverse=True)
            
            return scored_opportunities[:limit]
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get top opportunities: {e}")
            return []
    
    async def _monitor_network(self, network: str) -> None:
        """Monitor a specific network for new tokens."""
        logger.info(f"üîç Starting {network} token monitoring...")
        
        while self.active_monitors.get(network, False):
            try:
                # Get new pairs from DEXs
                new_tokens = await self._scan_network_for_new_tokens(network)
                
                # Process discovered tokens
                for token_data in new_tokens:
                    await self._process_discovered_token(token_data, network)
                
                # Update statistics
                self.discovery_stats.networks_monitored = len(self.active_monitors)
                self.discovery_stats.active_monitors = sum(self.active_monitors.values())
                
                # Wait before next scan
                await asyncio.sleep(self.discovery_interval)
                
            except Exception as e:
                logger.error(f"‚ùå Error monitoring {network}: {e}")
                await asyncio.sleep(60)  # Wait longer on error
        
        logger.info(f"üõë Stopped monitoring {network}")
    
    async def _scan_network_for_new_tokens(self, network: str) -> List[Dict[str, Any]]:
        """Scan network DEXs for new token pairs."""
        new_tokens = []
        
        try:
            # Get DEX sources for this network
            dex_list = self.dex_sources.get(network, [])
            
            for dex_name in dex_list:
    async def _scan_network_for_new_tokens(self, network: str) -> List[Dict[str, Any]]:
        """Scan network DEXs for new token pairs."""
        new_tokens = []
        
        try:
            # Get DEX sources for this network
            dex_list = self.dex_sources.get(network, [])
            
            for dex_name in dex_list:
                try:
                    # Scan specific DEX for new pairs
                    dex_tokens = await self._scan_dex_for_new_pairs(dex_name, network)
                    new_tokens.extend(dex_tokens)
                    
                except Exception as e:
                    logger.warning(f"Failed to scan {dex_name} on {network}: {e}")
                    continue
            
            # Remove duplicates and return
            unique_tokens = self._deduplicate_tokens(new_tokens)
            
            logger.debug(f"Found {len(unique_tokens)} new tokens on {network}")
            return unique_tokens
            
        except Exception as e:
            logger.error(f"‚ùå Network scan failed for {network}: {e}")
            return []
    
    async def _scan_dex_for_new_pairs(self, dex_name: str, network: str) -> List[Dict[str, Any]]:
        """Scan a specific DEX for new token pairs."""
        tokens = []
        
        try:
            # Mock implementation - replace with actual DEX API calls
            if dex_name == 'uniswap_v2':
                tokens = await self._scan_uniswap_v2(network)
            elif dex_name == 'uniswap_v3':
                tokens = await self._scan_uniswap_v3(network)
            elif dex_name == 'pancakeswap_v2':
                tokens = await self._scan_pancakeswap_v2(network)
            elif dex_name == 'sushiswap':
                tokens = await self._scan_sushiswap(network)
            else:
                # Generic DEX scanning
                tokens = await self._scan_generic_dex(dex_name, network)
            
            return tokens
            
        except Exception as e:
            logger.warning(f"DEX scan failed for {dex_name}: {e}")
            return []
    
    async def _scan_uniswap_v2(self, network: str) -> List[Dict[str, Any]]:
        """Scan Uniswap V2 for new pairs."""
        # Mock implementation - replace with actual Uniswap V2 scanning
        mock_tokens = [
            {
                "address": "0x1234567890123456789012345678901234567890",
                "symbol": "NEWTOKEN",
                "name": "New Token",
                "decimals": 18,
                "total_supply": "1000000000000000000000000",
                "creation_block": 18500000,
                "creator_address": "0xabcdef1234567890123456789012345678901234",
                "initial_liquidity_eth": "5.0",
                "dex_source": "uniswap_v2"
            }
        ]
        return mock_tokens
    
    async def _scan_uniswap_v3(self, network: str) -> List[Dict[str, Any]]:
        """Scan Uniswap V3 for new pairs."""
        # Mock implementation - replace with actual Uniswap V3 scanning
        return []
    
    async def _scan_pancakeswap_v2(self, network: str) -> List[Dict[str, Any]]:
        """Scan PancakeSwap V2 for new pairs."""
        # Mock implementation - replace with actual PancakeSwap scanning
        return []
    
    async def _scan_sushiswap(self, network: str) -> List[Dict[str, Any]]:
        """Scan SushiSwap for new pairs."""
        # Mock implementation - replace with actual SushiSwap scanning
        return []
    
    async def _scan_generic_dex(self, dex_name: str, network: str) -> List[Dict[str, Any]]:
        """Generic DEX scanning implementation."""
        # Mock implementation for other DEXs
        return []
    
    def _deduplicate_tokens(self, tokens: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove duplicate tokens from the list."""
        seen_addresses = set()
        unique_tokens = []
        
        for token in tokens:
            address = token.get('address', '').lower()
            if address and address not in seen_addresses:
                seen_addresses.add(address)
                unique_tokens.append(token)
        
        return unique_tokens
    
    async def _process_discovered_token(self, token_data: Dict[str, Any], network: str) -> None:
        """Process a newly discovered token."""
        try:
            token_address = token_data.get('address')
            
            # Skip if already processed
            if token_address in self.discovered_tokens:
                return
            
            # Create DiscoveredToken object
            discovered_token = DiscoveredToken(
                address=token_address,
                network=network,
                symbol=token_data.get('symbol', 'Unknown'),
                name=token_data.get('name', 'Unknown'),
                decimals=token_data.get('decimals', 18),
                total_supply=Decimal(str(token_data.get('total_supply', 0))),
                creation_block=token_data.get('creation_block', 0),
                creation_timestamp=datetime.utcnow(),  # Would calculate from block
                creator_address=token_data.get('creator_address', ''),
                initial_liquidity_eth=Decimal(str(token_data.get('initial_liquidity_eth', 0))),
                initial_liquidity_usd=float(token_data.get('initial_liquidity_usd', 0)),
                current_price_usd=float(token_data.get('current_price_usd', 0)),
                current_liquidity_usd=float(token_data.get('current_liquidity_usd', 0)),
                volume_1h=float(token_data.get('volume_1h', 0)),
                holder_count=token_data.get('holder_count', 0),
                transaction_count=token_data.get('transaction_count', 0),
                dex_pairs=[],
                discovery_source=token_data.get('dex_source', 'unknown')
            )
            
            # Store discovered token
            self.discovered_tokens[token_address] = discovered_token
            self.discovery_stats.total_discovered += 1
            
            logger.info(f"üîç New token discovered: {discovered_token.symbol} ({token_address})")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to process discovered token: {e}")
    
    async def _analyze_discovered_tokens(self) -> None:
        """Continuously analyze discovered tokens with AI."""
        logger.info("ü§ñ Starting AI analysis task...")
        
        while True:
            try:
                # Get unanalyzed tokens
                unanalyzed_tokens = [
                    token for token in self.discovered_tokens.values()
                    if not token.ai_analyzed
                ]
                
                # Process in batches
                for i in range(0, len(unanalyzed_tokens), self.analysis_batch_size):
                    batch = unanalyzed_tokens[i:i + self.analysis_batch_size]
                    
                    # Analyze batch
                    for token in batch:
                        await self._analyze_token_with_ai(token)
                    
                    # Brief pause between batches
                    await asyncio.sleep(1)
                
                # Wait before next analysis cycle
                await asyncio.sleep(60)  # Check every minute
                
            except Exception as e:
                logger.error(f"‚ùå AI analysis task error: {e}")
                await asyncio.sleep(60)
    
    async def _analyze_token_with_ai(self, token: DiscoveredToken) -> None:
        """Analyze a token with AI systems."""
        try:
            # Skip if already analyzed
            if token.ai_analyzed:
                return
            
            logger.debug(f"ü§ñ Analyzing {token.symbol} with AI...")
            
            # Honeypot detection
            if self.honeypot_detector:
                # Mock detection - replace with actual call
                is_honeypot = False  # await self.honeypot_detector.detect_honeypot(token.address, token.network)
                token.is_honeypot = is_honeypot
                
                if is_honeypot:
                    self.discovery_stats.honeypots_detected += 1
                    logger.warning(f"üçØ Honeypot detected: {token.symbol}")
            
            # Risk assessment
            if self.risk_assessor:
                # Mock risk assessment - replace with actual call
                risk_score = 3.5  # await self.risk_assessor.calculate_risk_score(token.address, token.network)
                token.risk_score = risk_score
            
            # Determine trading viability
            token.trading_viable = self._is_trading_viable(token)
            
            if token.trading_viable:
                self.discovery_stats.viable_opportunities += 1
                logger.info(f"‚úÖ Viable opportunity: {token.symbol} (Risk: {token.risk_score})")
            
            # Mark as analyzed
            token.ai_analyzed = True
            
        except Exception as e:
            logger.error(f"‚ùå AI analysis failed for {token.symbol}: {e}")
    
    def _is_trading_viable(self, token: DiscoveredToken) -> bool:
        """Determine if a token is viable for trading."""
        try:
            # Check honeypot status
            if token.is_honeypot:
                return False
            
            # Check risk score
            if token.risk_score and token.risk_score > 7.0:
                return False
            
            # Check liquidity
            if token.current_liquidity_usd < self.min_liquidity_threshold:
                return False
            
            # Check token age (not too old)
            age_hours = (datetime.utcnow() - token.creation_timestamp).total_seconds() / 3600
            if age_hours > self.max_token_age_hours:
                return False
            
            return True
            
        except Exception as e:
            logger.warning(f"Trading viability check failed: {e}")
            return False
    
    def _calculate_opportunity_score(self, token_data: Dict[str, Any]) -> float:
        """Calculate opportunity score for ranking."""
        try:
            score = 0.0
            
            # Risk score (lower is better)
            risk_score = token_data.get('risk_score', 5.0)
            score += max(0, 10 - risk_score) * 2  # 0-20 points
            
            # Liquidity score (higher is better)
            liquidity = token_data.get('liquidity_usd', 0)
            if liquidity > 100000:
                score += 20
            elif liquidity > 50000:
                score += 15
            elif liquidity > 25000:
                score += 10
            elif liquidity > 10000:
                score += 5
            
            # Volume score (higher is better)
            volume = token_data.get('volume_1h', 0)
            if volume > 50000:
                score += 15
            elif volume > 25000:
                score += 10
            elif volume > 10000:
                score += 5
            
            # Age score (newer is better, but not too new)
            age_minutes = token_data.get('age_minutes', 0)
            if 5 <= age_minutes <= 30:
                score += 15  # Sweet spot
            elif 30 < age_minutes <= 60:
                score += 10
            elif 60 < age_minutes <= 120:
                score += 5
            
            # Holder count score
            holders = token_data.get('holder_count', 0)
            if holders > 1000:
                score += 10
            elif holders > 500:
                score += 7
            elif holders > 100:
                score += 5
            elif holders > 50:
                score += 3
            
            return min(100.0, max(0.0, score))  # Cap at 100
            
        except Exception as e:
            logger.warning(f"Opportunity scoring failed: {e}")
            return 0.0
    
    async def get_discovery_statistics(self) -> Dict[str, Any]:
        """Get comprehensive discovery statistics."""
        try:
            # Calculate averages
            total_viable = sum(1 for token in self.discovered_tokens.values() if token.trading_viable)
            total_analyzed = sum(1 for token in self.discovered_tokens.values() if token.ai_analyzed)
            
            return {
                "discovery_stats": {
                    "total_discovered": self.discovery_stats.total_discovered,
                    "viable_opportunities": total_viable,
                    "honeypots_detected": self.discovery_stats.honeypots_detected,
                    "total_analyzed": total_analyzed,
                    "networks_monitored": len(self.monitoring_networks),
                    "active_monitors": sum(self.active_monitors.values()),
                    "analysis_completion_rate": (total_analyzed / max(1, self.discovery_stats.total_discovered)) * 100
                },
                "current_tokens": {
                    "total_stored": len(self.discovered_tokens),
                    "by_network": self._get_token_counts_by_network(),
                    "by_viability": {
                        "viable": total_viable,
                        "non_viable": len(self.discovered_tokens) - total_viable,
                        "pending_analysis": self.discovery_stats.total_discovered - total_analyzed
                    }
                },
                "configuration": {
                    "min_liquidity_threshold": self.min_liquidity_threshold,
                    "max_token_age_hours": self.max_token_age_hours,
                    "discovery_interval": self.discovery_interval,
                    "analysis_batch_size": self.analysis_batch_size
                },
                "performance": {
                    "average_discovery_time": self.discovery_stats.average_discovery_time,
                    "dex_sources": self.dex_sources
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Failed to get discovery statistics: {e}")
            return {}
    
    def _get_token_counts_by_network(self) -> Dict[str, int]:
        """Get token counts grouped by network."""
        counts = {}
        for token in self.discovered_tokens.values():
            network = token.network
            counts[network] = counts.get(network, 0) + 1
        return counts
    
    async def cleanup_old_tokens(self, max_age_hours: int = 24) -> int:
        """Remove old tokens from memory to prevent memory bloat."""
        try:
            cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
            old_tokens = []
            
            for address, token in self.discovered_tokens.items():
                if token.discovered_at < cutoff_time:
                    old_tokens.append(address)
            
            # Remove old tokens
            for address in old_tokens:
                del self.discovered_tokens[address]
            
            logger.info(f"üßπ Cleaned up {len(old_tokens)} old tokens")
            return len(old_tokens)
            
        except Exception as e:
            logger.error(f"‚ùå Token cleanup failed: {e}")
            return 0


# Factory function
async def create_token_discovery() -> TokenDiscovery:
    """Create and initialize token discovery service."""
    discovery = TokenDiscovery()
    await discovery.initialize()
    return discovery


# Utility function for testing
async def test_token_discovery():
    """Test token discovery functionality."""
    print("üß™ Testing Token Discovery Service...")
    
    try:
        discovery = await create_token_discovery()
        
        # Test monitoring for 30 seconds
        import asyncio
        
        async def test_monitoring():
            await discovery.start_monitoring(['ethereum'])
        
        # Run monitoring task with timeout
        try:
            await asyncio.wait_for(test_monitoring(), timeout=30.0)
        except asyncio.TimeoutError:
            await discovery.stop_monitoring()
        
        # Get statistics
        stats = await discovery.get_discovery_statistics()
        print(f"üìä Discovery Statistics: {stats}")
        
        print("‚úÖ Token Discovery test completed")
        return True
        
    except Exception as e:
        print(f"‚ùå Token Discovery test failed: {e}")
        return False


if __name__ == "__main__":
    asyncio.run(test_token_discovery())